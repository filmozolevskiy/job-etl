# CI Pipeline for Job-ETL Project
# This workflow runs on every push and pull request to ensure code quality
name: CI Pipeline

# Trigger conditions
on:
  push:
    branches:
      - main
      - 'feature/**'
      - 'bugfix/**'
  pull_request:
    branches:
      - main

# Cancel in-progress workflows when a new push is made to the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Job 1: Lint Python code with Ruff
  lint:
    name: Lint Python Code
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Check out the repository code
      - name: Checkout code
        uses: actions/checkout@v4
      
      # Step 2: Set up Python environment
      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'  # Cache pip dependencies for faster runs
      
      # Step 3: Install Ruff
      - name: Install Ruff
        run: pip install ruff
      
      # Step 4: Run Ruff linter
      - name: Run Ruff linter
        id: ruff
        run: |
          echo "üîç Running Ruff linter..."
          ruff check . --output-format=github 2>&1 | tee ruff-output.txt
          LINT_EXIT_CODE=${PIPESTATUS[0]}
          if [ $LINT_EXIT_CODE -ne 0 ]; then
            echo "lint-failed=true" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Upload lint output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ruff-output
          path: ruff-output.txt
          retention-days: 7
      
      # Step 5: Run Ruff formatter check (disabled for MVP - can re-enable later)
      # - name: Check code formatting
      #   run: |
      #     echo "üìù Checking code formatting..."
      #     ruff format --check .

  # Job 2: Run Python tests with pytest
  test:
    name: Run Python Tests
    runs-on: ubuntu-latest
    needs: lint  # Wait for linting to pass before running tests
    
    # Use a service container for PostgreSQL (needed for integration tests)
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: job_etl
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      # Step 1: Check out the repository code
      - name: Checkout code
        uses: actions/checkout@v4
      
      # Step 2: Set up Python environment
      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'
      
      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          echo "üì¶ Installing dependencies..."
          # Install test dependencies
          pip install pytest pytest-cov pytest-asyncio
          # Install database dependencies
          pip install psycopg2-binary sqlalchemy
          # Install service dependencies
          pip install requests pydantic python-dotenv
      
      # Step 3b: Initialize database tables
      - name: Bootstrap database
        env:
          PGPASSWORD: airflow
        run: |
          echo "üóÑÔ∏è Creating database tables..."
          psql -h localhost -U airflow -d job_etl -f scripts/bootstrap_db.sql
          echo "üßπ Ensuring clean database for tests..."
          psql -h localhost -U airflow -d job_etl -c "TRUNCATE TABLE raw.job_postings_raw CASCADE;"
      
      # Step 4: Run pytest with coverage
      - name: Run pytest
        id: pytest
        env:
          # Database connection for integration tests
          DATABASE_URL: postgresql://airflow:airflow@localhost:5432/job_etl
          # Add project root to Python path so services can be imported
          PYTHONPATH: .
        run: |
          echo "üß™ Running pytest..."
          # Check if services directory has Python files
          if [ -d "services" ] && [ "$(find services -name '*.py' -not -path '*/\.*' 2>/dev/null | wc -l)" -gt 0 ]; then
            # Services exist, run with coverage and strict requirements
            pytest tests/ --cov=services --cov-report=xml --cov-report=term-missing --cov-fail-under=0 -v --tb=short 2>&1 | tee pytest-output.txt
            TEST_EXIT_CODE=${PIPESTATUS[0]}
            if [ $TEST_EXIT_CODE -ne 0 ]; then
              echo "test-failed=true" >> $GITHUB_OUTPUT
              exit 1
            fi
          else
            # No services yet, run tests without coverage requirements
            echo "‚ö†Ô∏è  No service code found, running tests without coverage"
            pytest tests/ -v --tb=short 2>&1 | tee pytest-output.txt || echo "‚ö†Ô∏è  No tests found yet - this is expected for initial setup"
          fi
      
      - name: Upload test output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest-output
          path: pytest-output.txt
          retention-days: 7
      
      # Step 4b: Generate test failure summary
      - name: Generate test summary
        if: failure()
        run: |
          echo "## ‚ùå Test Failures" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Tests have failed. Next steps:" >> $GITHUB_STEP_SUMMARY
          echo "1. Read the pytest output above" >> $GITHUB_STEP_SUMMARY
          echo "2. Identify failing tests and error messages" >> $GITHUB_STEP_SUMMARY
          echo "3. Fix the code to make tests pass" >> $GITHUB_STEP_SUMMARY
          echo "4. Re-run CI to verify fixes" >> $GITHUB_STEP_SUMMARY
      
      # Step 5: Upload coverage report to GitHub
      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        if: always()  # Upload even if tests fail
        with:
          name: coverage-report
          path: |
            htmlcov/
            coverage.xml
          retention-days: 30

  # Job 3: Validate dbt models
  dbt-check:
    name: Validate dbt Models
    runs-on: ubuntu-latest
    needs: lint  # Can run in parallel with tests
    
    # Use PostgreSQL for dbt compile/test
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: airflow
          POSTGRES_PASSWORD: airflow
          POSTGRES_DB: job_etl
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      # Step 1: Check out the repository code
      - name: Checkout code
        uses: actions/checkout@v4
      
      # Step 2: Set up Python environment
      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'
      
      # Step 3: Install dbt with compatible protobuf version
      - name: Install dbt
        run: |
          echo "üì¶ Installing dbt..."
          pip install "protobuf>=3.20,<5" dbt-core==1.7.4 dbt-postgres==1.7.4
      
      # Step 4: Set up dbt profiles
      - name: Configure dbt profiles
        run: |
          echo "‚öôÔ∏è  Configuring dbt..."
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml << 'EOF'
          job_dbt:
            target: ci
            outputs:
              ci:
                type: postgres
                host: localhost
                user: airflow
                password: airflow
                port: 5432
                dbname: job_etl
                schema: public
                threads: 4
          EOF
      
      # Step 5: Create database schemas
      - name: Initialize database schemas
        env:
          PGPASSWORD: airflow
        run: |
          echo "üóÑÔ∏è  Creating database schemas..."
          psql -h localhost -U airflow -d job_etl -c "CREATE SCHEMA IF NOT EXISTS raw;"
          psql -h localhost -U airflow -d job_etl -c "CREATE SCHEMA IF NOT EXISTS staging;"
          psql -h localhost -U airflow -d job_etl -c "CREATE SCHEMA IF NOT EXISTS marts;"
      
      # Step 6: Install dbt dependencies
      - name: Install dbt packages
        working-directory: dbt/job_dbt
        run: |
          echo "üì¶ Installing dbt packages..."
          dbt deps --profiles-dir ~/.dbt
      
      # Step 7: Compile dbt models
      - name: Compile dbt models
        id: dbt-compile
        working-directory: dbt/job_dbt
        run: |
          echo "üî® Compiling dbt models..."
          dbt compile --profiles-dir ~/.dbt 2>&1 | tee ../../dbt-output.txt
          DBT_EXIT_CODE=${PIPESTATUS[0]}
          if [ $DBT_EXIT_CODE -ne 0 ]; then
            echo "dbt-failed=true" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Upload dbt output
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dbt-output
          path: dbt-output.txt
          retention-days: 7
      
      # Step 8: Run dbt tests
      - name: Run dbt tests
        working-directory: dbt/job_dbt
        run: |
          echo "üß™ Running dbt tests..."
          # For now, just parse - we'll add data tests later
          dbt parse --profiles-dir ~/.dbt
          echo "‚úÖ dbt models validated successfully!"

  # Job 4: Summary job (always runs, reports overall status)
  ci-summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [lint, test, dbt-check]
    if: always()  # Run even if previous jobs fail
    permissions:
      contents: read
      issues: write
      actions: read
    
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          path: artifacts/
      
      - name: Check CI status
        id: check-status
        run: |
          echo "## CI Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Lint | ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Test | ${{ needs.test.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| dbt Check | ${{ needs.dbt-check.result }} |" >> $GITHUB_STEP_SUMMARY
          
          # Determine if CI failed
          if [[ "${{ needs.lint.result }}" == "failure" ]] || \
             [[ "${{ needs.test.result }}" == "failure" ]] || \
             [[ "${{ needs.dbt-check.result }}" == "failure" ]]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "‚ùå **CI Pipeline Failed**" >> $GITHUB_STEP_SUMMARY
            echo "failed=true" >> $GITHUB_OUTPUT
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "‚úÖ **CI Pipeline Passed**" >> $GITHUB_STEP_SUMMARY
            echo "failed=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Create GitHub issue for CI failure
        if: steps.check-status.outputs.failed == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const sha = context.sha.substring(0, 7);
            const branch = context.ref.replace('refs/heads/', '');
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            
            // Build error summary
            let message = '‚ùå **CI Pipeline Failed**\n\n';
            message += `**Branch:** ${branch}\n`;
            message += `**Commit:** ${sha}\n`;
            message += `**Workflow Run:** ${runUrl}\n\n`;
            message += '## Failed Jobs\n\n';
            
            // Read artifacts for detailed errors
            const artifactsDir = 'artifacts';
            
            if ('${{ needs.lint.result }}' === 'failure') {
              message += '‚ùå **Lint Python Code**\n\n';
              try {
                const lintOutput = fs.readFileSync(path.join(artifactsDir, 'ruff-output', 'ruff-output.txt'), 'utf8');
                const errorLines = lintOutput.split('\n').slice(0, 50).join('\n'); // First 50 lines
                message += '<details>\n<summary>View linting errors</summary>\n\n```\n' + errorLines + '\n```\n</details>\n\n';
              } catch (e) {
                message += '*Error details not available*\n\n';
              }
            }
            
            if ('${{ needs.test.result }}' === 'failure') {
              message += '‚ùå **Run Python Tests**\n\n';
              try {
                const testOutput = fs.readFileSync(path.join(artifactsDir, 'pytest-output', 'pytest-output.txt'), 'utf8');
                // Extract failures section
                const lines = testOutput.split('\n');
                let failureStart = lines.findIndex(l => l.includes('FAILED') || l.includes('ERROR'));
                if (failureStart === -1) failureStart = 0;
                const errorLines = lines.slice(Math.max(0, failureStart - 5), failureStart + 100).join('\n');
                message += '<details>\n<summary>View test failures</summary>\n\n```\n' + errorLines + '\n```\n</details>\n\n';
              } catch (e) {
                message += '*Error details not available*\n\n';
              }
            }
            
            if ('${{ needs.dbt-check.result }}' === 'failure') {
              message += '‚ùå **Validate dbt Models**\n\n';
              try {
                const dbtOutput = fs.readFileSync(path.join(artifactsDir, 'dbt-output', 'dbt-output.txt'), 'utf8');
                const errorLines = dbtOutput.split('\n').slice(0, 50).join('\n');
                message += '<details>\n<summary>View dbt errors</summary>\n\n```\n' + errorLines + '\n```\n</details>\n\n';
              } catch (e) {
                message += '*Error details not available*\n\n';
              }
            }
            
            message += '\n[View full logs](' + runUrl + ')\n';
            
            // Check if issue already exists
            const { data: existingIssues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'ci-failure',
              per_page: 10
            });
            
            const alreadyExists = existingIssues.find(issue => 
              issue.body && (issue.body.includes(context.sha) || issue.body.includes(sha))
            );
            
            if (alreadyExists) {
              console.log(`Issue already exists: #${alreadyExists.number}`);
            } else {
              const newIssue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `üö® CI Failed on ${branch} (${sha})`,
                body: message,
                labels: ['ci-failure', 'bug', 'automated']
              });
              console.log(`‚úÖ Created issue #${newIssue.data.number} for AI agent to read`);
            }
      
      - name: Close resolved CI issues
        if: steps.check-status.outputs.failed == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const sha = context.sha.substring(0, 7);
            
            const { data: openIssues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'ci-failure'
            });
            
            for (const issue of openIssues) {
              if (issue.body && (issue.body.includes(context.sha) || issue.body.includes(sha))) {
                await github.rest.issues.update({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issue.number,
                  state: 'closed',
                  state_reason: 'completed'
                });
                
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: issue.number,
                  body: '‚úÖ CI Pipeline is now passing. Auto-closing.'
                });
                console.log(`Closed issue #${issue.number}`);
              }
            }
      
      - name: Fail if CI failed
        if: steps.check-status.outputs.failed == 'true'
        run: exit 1

